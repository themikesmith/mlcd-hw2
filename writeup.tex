%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%This is a science homework template. Modify the preamble to suit your needs. 
%The junk text is   there for you to immediately see how the headers/footers look at first 
%typesetting.


\documentclass[12pt]{article}

%AMS-TeX packages
\usepackage{amssymb,amsmath,amsthm} 
%geometry (sets margin) and other useful packages
\usepackage[margin=1.25in]{geometry}
\usepackage{graphicx,placeins}


%
%Redefining sections as problems
%
\makeatletter
\newenvironment{problem}{\@startsection
       {section}
       {1}
       {-.2em}
       {-3.5ex plus -1ex minus -.2ex}
       {2.3ex plus .2ex}
       {\pagebreak[3]%forces pagebreak when space is small; use \eject for better results
       \large\bf\noindent{Problem }
       }
       }
       {%\vspace{1ex}\begin{center} \rule{0.3\linewidth}{.3pt}\end{center}}
       \begin{center}\large\bf \ldots\ldots\ldots\end{center}}
\makeatother


%
%Fancy-header package to modify header/page numbering 
%
\usepackage{fancyhdr}
\pagestyle{fancy}
%\addtolength{\headwidth}{\marginparsep} %these change header-rule width
%\addtolength{\headwidth}{\marginparwidth}
\lhead{Problem \thesection}
\chead{} 
\rhead{\thepage} 
\lfoot{\small\scshape Machine Learning in Complex Domains} 
\cfoot{} 
\rfoot{\footnotesize PS \#2} 
\renewcommand{\headrulewidth}{.3pt} 
\renewcommand{\footrulewidth}{.3pt}
\setlength\voffset{-0.25in}
\setlength\textheight{648pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%
%Contents of problem set
%    
\begin{document}

\title{MLCD 2: Robots and Such }
\author{Elan Hourticolon-Retzler and Mike Smith}

\maketitle

\thispagestyle{empty}

\begin{problem}{Parameter Sharing}


4.1.4 - Deliverables for estimate-params

Our estimate-params program is written in Java.  We have a driver 
program, a singleton Network class, a MotionModel class, an 
ObservationModel class, and a Constants class.

The network class has three methods, each of which are called in 
succession by the driver: read, train, and write.

In the read function we use assumptions specified about our models and 
the input format to determine sufficient statistics for each network; 
they are the number of rows I, the number of columns J, the number of 
landmarks L, and the number of time steps T.  We collect these in the 
method while reading the network file.

In the train function, we initialize our models with the applicable 
sufficient statistics.  We note that to compute the MAP estimate using
add-1 smoothing, we need to only maintain counts of times we observed 
each parameter p, and maintain counts of all possible instances where 
we could have observed said parameter p.  To implement add-1 smoothing, 
We initialize our observed counts of each p to 1, and our counts of 
chances of observation to 2.
For the motion model, we maintain two 4x1 arrays, one for successful 
moves in each of the 4 directions, and one for attempted moves in each 
of the 4 directions.
For the observation model, we maintain a four-dimensional array.  We 
have an IxJ array, and in each cell we store a two dimensional array 
which is information about observing walls and landmarks at that cell 
of the map.  This 2-d array is (4(1+L))x2, as for each cell we could 
observe a wall in each of the 4 directions, and we could observe any of
 the L landmarks in each of the 4 directions.  The remaining dimension 
x2 is to store the number of times we made observation $O_i$ at the cell 
(i,j), and the number of times we visited the cell (i,j) respectively.

In the writeCPD function, we loop over all time steps in the network.
For each time step, we output the same probabilities since we are 
sharing parameters.  For each time step, we output the results of the 
motion model, and those of the observation model.
For the motion model, we loop once over every previous row value.  For 
each of these values, we loop over every possible action, and print the
probability of the action's success given the previous row value, and
print the probability of the action's failure (if applicable).  
Analogously, we loop over every previous column value, and for each of 
these values, we loop over every possible action.  We print the 
probability of the action's success given the previous column value, and
print the probability of the action's failture (if applicable).
For the observation model, we loop once over every possible value of 
(i,j), and for each observation $O_i$, print the probability of making 
that observation ('yes'), and the probability of not making it ('no').

4.1.5  Analytical questions about shared parameterization

1. An important advantage of sharing parameters is the lesssening of the
effects of overfitting.  Explain why.  Why is this issue important when
doing parameter estimation in this assignment?

Sharing parameters lessens the effects of overfitting because we smooth
across all time steps, computing probabilities using data from all time
 steps.  By doing this, we increase our sample size.  If we did not 
share parameters, we would be computing probabilities of events at a 
given time, and we would need many more training trajectories both to 
accurately estimate probabilities, and to avoid many scenarios for which
we have no data.  Smoothing takes care of this, but the specific brand 
of smoothing that we use, add-1, does not adjust for the sample size, so
we end up smoothing an incredible amount relative to our unseen event.

This issue is important when doing our estimation in this assignment 
because we want our probabilities in the CPD to be meaningful, and to 
carry weight.  We also want to be able to predict the chance of an event
given a context that we never encounter (eg, transitioning from a 
position at a given time step after a move from a previous position in 
the previous time step), so it is important that we smooth, and assume 
that this probability is equal to the probability of being in a 
position after a move from a previous position.

2. Describe at least one property of the robot localization setting (with regard
to either the motion model or observation model) that you cannot model with the shared
parameterization in this assignment, but that you could model if we used an explicit
parameterization (i.e. we learn the entire CPT for particular positions or time steps). It
does not have to be a property of the environment that we described in this handout; you
can construct something relevant that you might like to model in this setting.

If the robot's legs wear out after a certain number of moves, the 
chances of a move failing greatly increase after time is greater than
this threshold.  Sharing parameters across all time steps doesn't allow
for us to capture this.

3. Describe a way to combine the advantages of both approaches. That is, come
up with a model design that can potentially learn unique values for all CPT entries, but
also has shared parameters which reduce overfitting. Sketch the idea in a few sentences,
and explain the advantages over the other models.\\

??? //TODO



\end{problem}{}

\begin{problem}{Inference}
4.2.1\\

1 and 3.\\

Describe the process you followed to make the clique tree. Note one or two
points in the process where you could have obtained a (slightly) different clique tree by
making different choices.\\

We noted that because of the problem domain and structure of the models,
we only needed the sufficient statistics from a network to be able to 
construct a clique tree for that network.  Given these statistics, we 
know how to convert the network to an undirected chordal graph and thus
get the set of maximal cliques.  They are as follows. \\
\textit{Observation Model:}\\
for every value of time $T=t_i$:\\
\indent	for each direction:\\
\indent \indent row t, col t, observe wall d t\\
\indent	for each direction:\\
\indent	\indent	for each landmark:\\
\indent	\indent	\indent	row t, col t, observe landmark L d t\\
\textit{Motion Model:}\\
for every value of time $T=t_i$, save $T=t_n$:\\
\indent	row t, row t+1, col t+1, action t\\
\indent	row t, col t, col t+1, action t\\
if $t=t_n$: \\
\indent	$Action_t$ \\

\noindent Note that we discussed with Dr Saria, and she advised us to switch to cliques of the following form given our motion model, even though they are not derived using the minimal triangulation of the graph:\\
\textit{Motion Model:}\\
for every value of time $T=t_i$, save $T=t_n$:\\
\indent	row t, col t+1, row t+1, col t+1, action t\\
if $t=t_n$: \\
\indent	$Action_t$ \\
\textit{Our clique trees built using the former method with minimal triangulation are in 'cliquetree-*.txt' files.  This naming convention contrasts with that used for our trees built using the latter method with larger cliques, which is 'cliquetree-*\_big.txt'.}\\
Given that we have these maximal cliques, create a cluster graph by 
going through all possible pairs $i,j$ of cliques (except where $i=j$), 
adding an edge ($i$ to $j$) to the graph if the number of variables in 
common between $i$ and $j$ is greater than 0.  The weight of each edge is
the number of variables in common along that edge.\\

We sort these edges in decreasing order by weight so that we can apply
a modified version of Kruskal's algorithm for finding a maximum spanning
tree.  We modify it because we note that our clique tree may not be 
fully connected, and that's okay.\\

Once we have our tree, we print it according to the output format.\\

There was one choice we made to make when creating a tree, such that if
we had taken a different path the tree would have differed.  For the 
variables at each time step, we had to choose between creating one of 
two new edges when generating the list of maximal cliques.  We close to
connect $Row_t$ to $Col_{t+1}$ where we instead could have chosen to make 
an edge between $Col_t$ and $Row{t+1}$.  This would affect our cliques, and
would therefore affect the vertices and edges of our clique tree.\\

2. [2 points] Demonstrate that the running intersection property holds 
in the resulting clique tree. A formal proof is not required, simply 
explain how this property holds.\\

\noindent The form of our clique tree is as follows:\\
\noindent \textit{Using our minimal triangulation trees:}\\
We note that this is simply a chain.  At each time $t$, we have a clique $G_t$ (row t, col t, action t, col t+1) connected to a clique $B_t$ (row t, action t, row r+1, col t+1), and a clique $G_t$ connected to cliques $R_{t}^{i}$ (row t, col t, observation i t).  $B_t$ connects to $G_{t+1}$ in the next time step.\\
\noindent \textit{Using our trees with larger cliques:}\\
We note that this is simply a chain, with each large clique at time $t$ (row t, col t, action t, row t+1, col t+1) having edges leading to the observations at time $t$ (row t, col t, observation $i$ at time t), and the next large clique at time $t+1$ (row t+1, col t+1, action t+1, row t+2, col t+2).\\

\noindent Because our cliques repeat the same structure across time steps, 
we can restrict our 'proof' to examining a small subset of contiguous
time steps in the clique tree, and by induction we can see that for the
whole tree it is true.  The final action $Action_t$ is a clique of size 1,
such that there are no edges, so the running intersection property holds. \\

4.\\

\noindent For a grid of size N by M and a sequence of T actions and observations (assume they are fully observed), what would be the computational complexity of computing the distribution over the final position at time T if we simply marginalized over a joint CPT?
What is the computational complexity of computing the distribution over the final position at time T using message passing in your clique tree?\\
The computational complexity of computing the distribution over the final position at time T if we simply marginalize over a joint CPT is as follows:\\
If we have $n$ variables, and the maximum cardinality out of all the variables is $c$, then the cost of marginalizing over the joint CPT is $c^n$.\\
The computational complexity of the clique tree message passing is as follows, from p358 of Koller:
Letting $c$ be the execution cost of message passing in a clique tree to one root (upward pass), the cost of obtaining the calibrated tree is $2c$.  One pass of the message passing algorithm involves every vertex sending a message to its 'downstream' neighbors, so it involves $v$ vertices.  When sending to vertex $j$, each vertex $i$ marginalizes out all its variables not in the sepset between its variables and $j$'s variables.  The number of variables marginalized is on the order of the size of the largest clique, which we'll call $k$.\\
We are given all actions and observations, as incremental evidence.  We could do it one of two ways.\\
\textit{We note that at most we have 3 vertices for each time step, if we use our minimal triangulation, and the largest clique size is 3.  This would make $v = 3T$, and $k = 3$.  We also could have at most 2 vertices for each time step, and the largest clique size is 5, which would make $v = 2T$ and $k = 5$.  We consider the latter case to be the worst case scenario, as we will see.}\\
\noindent - Let the cost of an operation on a factor be proportional to the size of the factor.  We could calibrate our tree based on our distribution's initial beliefs, with cost $2c = 2vk$.  Note that $k$ is the size of the largest clique, which means the size of the largest factor is $O(k)$.  We could then introduce our evidence set $E$, which has $O(T)$ actions, and $O(T n)$ observations, where $T$ is the number of time steps, and $n$ is the maximum number of observations at any time step.  We would therefore need to multiply in $O(2 T n)$ indicator functions, which has cost $O(2 T n x)$, where $x$ is the time needed to find an applicable clique at which to multiply any given indicator function.  We then would need to recalibrate the tree, which adds cost $2 v k$ again.  This is a total cost of $O(4 v k)$, which in our worst case scenario is $O(4 \cdot 2T \cdot 5) = O(40T)$.\\
\noindent - Assuming we only want to answer this one query, we can forego calibrating the clique tree first, and only calibrate it once after we multiply in the indicator factors with the evidence.  This brings us down to $O(20T)$.\\

5. Suppose we wanted to query for the robot state at time 5 and time 15.\\
(a) How would you modify the clique tree so that you could make this query?
Prove that your modification is guaranteed to give the correct marginal probability.
Hint: to ask a query, there must exist a cluster which contains all of the query variables.\\
To ask a query, there must exist a cluster which contains all variables in the query.  We assume a robot state at time t means asking about (row t, col t, action t, observations t i).  Therefore, there must exist a cluster which contains (row 5, col 5, action 5, observations 15 i, row 15, col 15, action 15, observations 15 i).  We would therefore combine our previous cliques in our tree that involve time states 5 and 15 into one big clique.  Using either method of generating clique trees (using our minimal triangulation, or using our larger cliques), we note that in the resulting graph there would be a cycle.  So, taking the section of the graph with subgraphs from time $6 < t < 14$, we simply erase an edge between any two subgraphs.  Our modification is guaranteed to give the correct marginal because when the clique tree is calibrated, we are guaranteed that it is an alternative representation of the joint measure, one that directly reveals the clique marginals (from p 364 of Koller).  If we examine our uber-clique (5,15), that holds all variables in question, therefore we are examining the marginal of the joint when marginalizing out all other variables.\\
(b) Why is it not always valid to answer this type of query using two different clusters where each contains one of the query variables? \\
From p370 of Koller, if we have a query $P(\textbf{Y} | e)$ where the variables $\textbf{Y}$ are not present together in a single clique, we can combine them into a clique, but in doing so we negate many of the advantages of our clique tree algorithm, because this construction only allows us to answer our specific given query.\\
We can use 'out of clique inference in a clique tree' from p371, which considers only the subtree that contains the variables in question, which is the joint which contains the subset of variables in question.  We then perform variable elimination to eliminate the variables not in question.\\
We may not just use the two different clusters where each contains a subset of the query variables, because then we lose information from the joint.  If we have query variables $\textbf{Y}$, and we find cliques $C_1 \ldots C_n$ that correspond to variable subsets $Y_1 \ldots Y_n$, then each clique $C_i$ contains the joint distribution, with all but the variables in $C_i$ marginalized out.  Combining marginals is not equal to combining the joint, if the marginals are not independent.  If they are independent, it's fine, as we know that $P(C_1,C_2,\ldots,C_n) = \prod_{C_i} P(C_i)$.\\

4.2.5 -- Empirical Questions : Message Passing\\

\noindent 1. Using your bayes-query-sp program, determine the distribution over the final position of the robot given a sequence of actions and a sequence of observations. Loosely, you'll be inferring the distribution: p(final position $\mid$ action sequence, observation sequence). For each of $T \in {10,100,1000}$, construct a set of queries where the lefthand side contains different values of PositionCol T and PositionRow T. You should use the provided evidence (i.e. righthand side) given in the file evidence-q1-grid10x10-tT.txt, which corresponds to the model in network-grid10x10-tT.txt. Notice that some of the evidence is missing! Apparently, our robot had a bug in which it failed to report its action and observations every fifth timestep. Not to worry, you can just marginalize over these missing data points. Describe in what region of the map the final position of the robot is likely to be.\\
Time = 10: the robot is equally likely to end up in any position?\\

2.\\

3.\\

Feed the query into the algorithm.  We need to make sure that the cliques that we create contain the variables (row t, col t, row t+1, col t+1).  Calibrate the tree, then whe algorithm will introduce indicator factors for each evidence given.  It will then recalibrate the tree, and then we can find the clique in question on the LHS and marginalize out the action variable.\\

4.\\
To compute the log likelihood of the evidence, of a full trajectory, one must feed that evidence into the algorithm, which will then multiply in indicator factors for every single variable.  We then calibrate our tree.  To get the joint, ?? \\

5.\\
A design for an experiment about the time saved with our incremental update code is that it saves us passes of the algorithm.  We check if we need to calibrate.  If we do, we only need to do one pass.  Without this incremental update code, we would be adding in things and recalibrating for every single query.  We compose queries that add incremental evidence, and check the time difference.\\
\noindent results:\\
\noindent \textit{incremental updates on:}\\
time ./bayes-query-sp hw2-files/network-grid10x10-t10.txt hw2-files/cpd-grid10x10-t10.txt hw2-files/cliquetree-grid10x10-t10\_big.txt queries/queries-q5-grid10x10-t10.txt -s -i $>$ q5-results-inc-on.txt\\
8.179u 1.204s 0:06.39 146.6\%    0+0k 0+80io 0pf+0w\\
\noindent \textit{incremental updates off:}\\
time ./bayes-query-sp hw2-files/network-grid10x10-t10.txt hw2-files/cpd-grid10x10-t10.txt hw2-files/cliquetree-grid10x10-t10\_big.txt queries/queries-q5-grid10x10-t10.txt -s -n  $>$ q5-results-inc-off.txt\\
32.041u 1.232s 0:29.52 112.7\%   0+0k 0+112io 0pf+0w\\

We can see that without the incremental updates, processing the queries takes longer.  This was using a 50-line query, on the smallest network, with each subsequent query adding one piece of incremental evidence.  One can imagine that with larger networks, the effect would be magnified.

\end{problem}{}

\begin{problem}{Bayesian Score for Bayesian Networks}

\begin{eqnarray*}
P( \mathcal{D} \mid \mathcal{G}) & = & 
\prod_i \prod _{\mathbf{u_{i}} \in Val(Pa^{\mathcal{G}}_{X_{i}})} 
\frac{\Gamma(\alpha^{\mathcal{G}}_{X_{i}\mid \mathbf{u}_{i}})}{\Gamma(\alpha^{\mathcal{G}}_{X_{i}\mid \mathbf{u}_{i}} + M[ \mathbf{u}_{i}])} 
\prod_{x_{i}^{j} \in Val(X_{i})} 
\left[ \frac{\Gamma(\alpha^{\mathcal{G}}_{x_{i}^{j}\mid \mathbf{u}_{i}} + M[ x_{i}^{j},\mathbf{u}_{i}])}{\Gamma(\alpha^{\mathcal{G}}_{x_{i}^{j}\mid \mathbf{u}_{i}} )} \right]\\
& = & 
\prod_i \prod _{\mathbf{u_{i}} \in Val(Pa^{\mathcal{G}}_{X_{i}})} 
A
\prod_{x_{i}^{j} \in Val(X_{i})} 
B\\
\end{eqnarray*}

\begin{eqnarray*}
p_{i} & = & \alpha^{\mathcal{G}}_{X_{i}\mid \mathbf{u}_{i}}\\
p_{i,j} & = & \alpha^{\mathcal{G}}_{x_{i}^{j}\mid \mathbf{u}_{i}}   \\
p_{i} & = & \sum_{x_{i}^{j} \in Val(X_{i})} p_{i,j}\\
\end{eqnarray*}


\begin{eqnarray*}
\ell(\hat{\mathbf{\theta}},\mathcal{D}) & = & 
	\sum^{n}_{i=1} \left[ 
	\sum_{\mathbf{u_{i}} \in Val(Pa^{\mathcal{G}}_{X_{i}})}  
	\sum_{x_{i}^{j} \in Val(X_{i})}  
	M[x_{i},\mathbf{u_{i}}] \log \hat{\mathbf{\theta}}_{x_{i} \mid \mathbf{u_{i}}} \right]
\end{eqnarray*}


\begin{eqnarray*}
A & = & \frac{\Gamma(\alpha^{\mathcal{G}}_{X_{i}\mid \mathbf{u}_{i}})}{\Gamma(\alpha^{\mathcal{G}}_{X_{i}\mid \mathbf{u}_{i}} + M[ \mathbf{u}_{i}])} \\
& = & \frac{\Gamma(p_{i})}{\Gamma(p_{i} + M[ \mathbf{u}_{i}])} \\
& = & \frac{p_{i}^{(p_{i}-\frac{1}{2})} e^{-p_{i}}}{(p_{i} + M[ \mathbf{u}_{i}])^{(p_{i} + M[ \mathbf{u}_{i}] - \frac{1}{2})}e^{-(p_{i} + M[ \mathbf{u}_{i}]})} \\
& = & \frac{p_{i}^{(p_{i}-\frac{1}{2})} }{(p_{i} + M[ \mathbf{u}_{i}])^{(p_{i} + M[ \mathbf{u}_{i}] - \frac{1}{2})}} e^{-p_{i}+(p_{i} + M[ \mathbf{u}_{i}])} \\
& = & \frac{p_{i}^{(p_{i}-\frac{1}{2})} }{(p_{i} + M[ \mathbf{u}_{i}])^{(p_{i} + M[ \mathbf{u}_{i}] - \frac{1}{2})}} e^{ M[ \mathbf{u}_{i}]} \\
& = & p_{i}^{(p_{i}-\frac{1}{2})} 
	(p_{i} + M[ \mathbf{u}_{i}])^{-(p_{i} + M[ \mathbf{u}_{i}] - \frac{1}{2})} 
	e^{ M[ \mathbf{u}_{i}]} \\
\log A & = & (p_{i}-\frac{1}{2})\log p_{i}
	-(p_{i} + M[ \mathbf{u}_{i}] - \frac{1}{2})  \log (p_{i} + M[ \mathbf{u}_{i}])
	+ M[ \mathbf{u}_{i}] \\
\end{eqnarray*}

\begin{eqnarray*}
B & = & \frac{\Gamma(\alpha^{\mathcal{G}}_{x_{i}^{j}\mid \mathbf{u}_{i}} + M[ x_{i}^{j},\mathbf{u}_{i}])}{\Gamma(\alpha^{\mathcal{G}}_{x_{i}^{j}\mid \mathbf{u}_{i}} )}\\
& = & \frac{\Gamma(q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}])}{\Gamma(q_{i,j} )}\\
& = & \frac{(q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}])^{(q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}] -\frac{1}{2})}e^{-(q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}])}}{q_{i,j} ^{(q_{i,j} -\frac{1}{2})} e^{-q_{i,j} }}\\
& = & \frac{(q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}])^{(q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}] -\frac{1}{2})}}{q_{i,j} ^{(q_{i,j} -\frac{1}{2})} } 
	e^{-M[ x_{i}^{j},\mathbf{u}_{i}]}\\
& = & (q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}])^{(q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}] -\frac{1}{2})}
	q_{i,j} ^{-(q_{i,j} -\frac{1}{2})} 
	e^{-M[ x_{i}^{j},\mathbf{u}_{i}]}\\
\log B & = & {(q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}] -\frac{1}{2})} \log (q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}])
	-(q_{i,j} -\frac{1}{2}) q_{i,j} 
	-M[ x_{i}^{j},\mathbf{u}_{i}]\\
\end{eqnarray*}
\\
\begin{eqnarray*}
\log P( \mathcal{D} \mid \mathcal{G}) & = & 
	\sum_{i} \sum_{\mathbf{u_{i}} \in Val(Pa^{\mathcal{G}}_{X_{i}})} 
	\left[ \log A + 
	\sum_{x_{i}^{j} \in Val(X_{i})} 
	\log B \right] \\
& = & 
	\sum_{i} \sum_{\mathbf{u_{i}} \in Val(Pa^{\mathcal{G}}_{X_{i}})} 
	(p_{i}-\frac{1}{2})\log p_{i}
	-(p_{i} + M[ \mathbf{u}_{i}] - \frac{1}{2})  \log (p_{i} + M[ \mathbf{u}_{i}])
	+ M[ \mathbf{u}_{i}]\\
	&+& \sum_{x_{i}^{j} \in Val(X_{i})} 
	{(p_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}] -\frac{1}{2})} \log (p_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}])
	-(p_{i,j} -\frac{1}{2}) p_{i,j} 
	-M[ x_{i}^{j},\mathbf{u}_{i}]\\
& = & 
	\sum_{i} \sum_{\mathbf{u_{i}} \in Val(Pa^{\mathcal{G}}_{X_{i}})} 
	(p_{i}-\frac{1}{2})\log p_{i}
	-(p_{i} + M[ \mathbf{u}_{i}] - \frac{1}{2})  \log (p_{i} + M[ \mathbf{u}_{i}])\\
	&+& \sum_{x_{i}^{j} \in Val(X_{i})} 
	{(p_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}] -\frac{1}{2})} \log (p_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}])
	-(p_{i,j} -\frac{1}{2}) p_{i,j} \\
\end{eqnarray*}

As $M \rightarrow \infty$, the terms of p drop out. Also $M \log M$ terms beat out $\log M$ terms leaving us with:

\begin{eqnarray*}
\log P( \mathcal{D} \mid \mathcal{G}) 
& = & O(1) + \sum_{i} \sum_{\mathbf{u_{i}} \in Val(Pa^{\mathcal{G}}_{X_{i}})} 
	-M[ \mathbf{u}_{i}] \log (M[ \mathbf{u}_{i}])\\
	&+& \sum_{x_{i}^{j} \in Val(X_{i})} 
	M[ x_{i}^{j},\mathbf{u}_{i}] \log (M[ x_{i}^{j},\mathbf{u}_{i}]) \\
& = & \sum_{i} \sum_{\mathbf{u_{i}} \in Val(Pa^{\mathcal{G}}_{X_{i}})} \sum_{x_{i}^{j} \in Val(X_{i})} 
	M[ x_{i}^{j},\mathbf{u}_{i}] \log (M[ x_{i}^{j},\mathbf{u}_{i}])\\
	& - &\sum_{i} \sum_{\mathbf{u_{i}} \in Val(Pa^{\mathcal{G}}_{X_{i}})} \sum_{x_{i}^{j} \in Val(X_{i})} 
	M[ \mathbf{u}_{i}] \log (M[ \mathbf{u}_{i}])\\
& = & \ell (\hat{\theta}: \mathcal{D}) -  \frac{Dim[\mathcal{G}]}{2} \log M + O(1)\\
\end{eqnarray*}
Where the first term is by definition. The second terms follows from the realization that we're only summing over the parents of $X_i$
\\   
\\ 
\\   
%\\
%\begin{eqnarray*}
%\log P( \mathcal{D} \mid \mathcal{G}) & = & 
%	\sum_{i} \sum_{\mathbf{u_{i}} \in Val(Pa^{\mathcal{G}}_{X_{i}})} 
%	\left[ \log A + 
%	\sum_{x_{i}^{j} \in Val(X_{i})} 
%	\log B \right] \\
%& = & 
%	\sum_{i} \sum_{\mathbf{u_{i}} \in Val(Pa^{\mathcal{G}}_{X_{i}})} 
%	\frac{1}{2} \log (2 \pi) (p_{i}) \log p_{i}  - \frac{1}{2} \log p_{i} - p_{i} \\
%	& - &\frac{1}{2} \log (2 \pi)  (p_{i} + M[ \mathbf{u}_{i}]) \log  (p_{i} + M[ \mathbf{u}_{i}])  + \frac{1}{2} \log (p_{i} + M[ \mathbf{u}_{i}]) +(p_{i} + M[ \mathbf{u}_{i}]) \\
%	& + &\sum_{x_{i}^{j} \in Val(X_{i})} 
%	\frac{1}{2} \log (2 \pi) ((q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}])) \log (q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}])\\
%	& - &\frac{1}{2} \log (q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}]) - (q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}]) \\
%	& - &\frac{1}{2} \log (2 \pi) q_{i,j} \log q_{i,j}  + \frac{1}{2} \log q_{i,j}+ q_{i,j} \\
%\end{eqnarray*}
%
%The $(q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}])$ cancels with $(p_{i} + M[ \mathbf{u}_{i}])$, we also can factor out the $\frac{1}{2}$
%
%\begin{eqnarray*}
%\log P( \mathcal{D} \mid \mathcal{G}) & = & 
%	\frac{1}{2} \sum_{i} \sum_{\mathbf{u_{i}} \in Val(Pa^{\mathcal{G}}_{X_{i}})} 
%	 \log (2 \pi) (p_{i}) \log p_{i}  -  \log p_{i} \\
%	& - & \log (2 \pi)  (p_{i} + M[ \mathbf{u}_{i}]) \log  (p_{i} + M[ \mathbf{u}_{i}])  + \log (p_{i} + M[ \mathbf{u}_{i}])  \\
%	& + &\sum_{x_{i}^{j} \in Val(X_{i})} 
%	 \log (2 \pi) ((q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}])) \log (q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}])\\
%	& - & \log (q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}]) \\
%	& - &\log (2 \pi) q_{i,j} \log q_{i,j}  +\log q_{i,j}\\
%& = & 
%	\frac{1}{2} \sum_{i} \sum_{\mathbf{u_{i}} \in Val(Pa^{\mathcal{G}}_{X_{i}})} 
%	 \log (2 \pi) (p_{i}) \log p_{i}  -  \log p_{i} \\
%	& - & \log (2 \pi)  (p_{i} + M[ \mathbf{u}_{i}]) \log  (p_{i} + M[ \mathbf{u}_{i}])  + \log (p_{i} + M[ \mathbf{u}_{i}])  \\
%	& + &\sum_{x_{i}^{j} \in Val(X_{i})} 
%	 \log (2 \pi) ((q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}])) \log (q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}])\\
%	& - & \log (q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}]) \\
%	& - &\log (2 \pi) q_{i,j} \log q_{i,j}  +\log q_{i,j}\\
%\end{eqnarray*}
%
%As $M \rightarrow \infty$ we have $M[ x_{i}^{j},\mathbf{u}_{i}] \rightarrow \hat{\theta}$:
%
%\begin{eqnarray*}
%& = & 
%	\frac{1}{2} \sum_{i} \sum_{\mathbf{u_{i}} \in Val(Pa^{\mathcal{G}}_{X_{i}})} 
%	 \log (2 \pi) (p_{i}) \log p_{i}  -  \log p_{i} \\
%	& - & \log (2 \pi)  (p_{i} + M[ \mathbf{u}_{i}]) \log  (p_{i} + M[ \mathbf{u}_{i}])  + \log (p_{i} + M[ \mathbf{u}_{i}])  \\
%	& + &\sum_{x_{i}^{j} \in Val(X_{i})} 
%	 \log (2 \pi) ((q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}])) \log (q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}])\\
%	& - & \log (q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}]) \\
%	& - &\log (2 \pi) q_{i,j} \log q_{i,j}  +\log q_{i,j}\\
%\end{eqnarray*}
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\\
%\begin{eqnarray*}
%\log P( \mathcal{D} \mid \mathcal{G}) & = & 
%	\sum_{i} \sum_{\mathbf{u_{i}} \in Val(Pa^{\mathcal{G}}_{X_{i}})} 
%	\left[ \log A + 
%	\sum_{x_{i}^{j} \in Val(X_{i})} 
%	\log B \right] \\
%& = & 
%	\sum_{i} \sum_{\mathbf{u_{i}} \in Val(Pa^{\mathcal{G}}_{X_{i}})} 
%	 \log [ p_{i}^{(p_{i}-\frac{1}{2})} ] 
%	+ \log [(p_{i} + M[ \mathbf{u}_{i}])^{-(p_{i} + M[ \mathbf{u}_{i}] - \frac{1}{2})} ]
%	+ M[ \mathbf{u}_{i}]\\
%	&+& \sum_{x_{i}^{j} \in Val(X_{i})} 
%	\log [(q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}])^{(q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}] -\frac{1}{2})}] 
%	+ \log q_{i,j} ^{-(q_{i,j} -\frac{1}{2})}  
%	-M[ x_{i}^{j},\mathbf{u}_{i}]\\
%& = & 
%	\sum_{i} \sum_{\mathbf{u_{i}} \in Val(Pa^{\mathcal{G}}_{X_{i}})} 
%	 (p_{i}-\frac{1}{2}) \log [ p_{i} ] 
%	-(p_{i} + M[ \mathbf{u}_{i}] - \frac{1}{2}) \log [(p_{i} + M[ \mathbf{u}_{i}]) ]
%	+ M[ \mathbf{u}_{i}]\\
%	&+ &\sum_{x_{i}^{j} \in Val(X_{i})} 
%	(q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}] -\frac{1}{2}) \log [(q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}])] 
%	-(q_{i,j} -\frac{1}{2}) \log q_{i,j}   
%	-M[ x_{i}^{j},\mathbf{u}_{i}]\\
%& = & 
%	\sum_{i} \sum_{\mathbf{u_{i}} \in Val(Pa^{\mathcal{G}}_{X_{i}})} 
%	 (p_{i}-\frac{1}{2}) \log [ p_{i} ] 
%	-(p_{i} + M[ \mathbf{u}_{i}] - \frac{1}{2}) \log [(p_{i} + M[ \mathbf{u}_{i}]) ]
%	+ M[ \mathbf{u}_{i}]\\
%	&+ &\sum_{x_{i}^{j} \in Val(X_{i})} 
%	(q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}] -\frac{1}{2}) \log [(q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}])] 
%	-(q_{i,j} -\frac{1}{2}) \log q_{i,j}   
%	-M[ x_{i}^{j},\mathbf{u}_{i}]\\	
%& = & 
%	\sum_{i} \sum_{\mathbf{u_{i}} \in Val(Pa^{\mathcal{G}}_{X_{i}})} 
%	 (p_{i}-\frac{1}{2}) \log [ p_{i} ] 
%	-(p_{i} + M[ \mathbf{u}_{i}] - \frac{1}{2}) \log [(p_{i} + M[ \mathbf{u}_{i}]) ]\\
%	&+ &\sum_{x_{i}^{j} \in Val(X_{i})} 
%	(q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}] -\frac{1}{2}) \log [(q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}])] 
%	-(q_{i,j} -\frac{1}{2}) \log q_{i,j}  \\
%\end{eqnarray*}
%
%If we let $ M \rightarrow \inf$ then the alpha terms fall out and we are left with:
%
%\begin{eqnarray*}
%\log P( \mathcal{D} \mid \mathcal{G}) & = & 
%	\sum_{i} \sum_{\mathbf{u_{i}} \in Val(Pa^{\mathcal{G}}_{X_{i}})} 
%	 (p_{i}-\frac{1}{2}) \log [ p_{i} ] 
%	-(p_{i} + M[ \mathbf{u}_{i}] - \frac{1}{2}) \log [(p_{i} + M[ \mathbf{u}_{i}]) ]\\
%	&+ &\sum_{x_{i}^{j} \in Val(X_{i})} 
%	(q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}] -\frac{1}{2}) \log [(q_{i,j}  + M[ x_{i}^{j},\mathbf{u}_{i}])] 
%	-(q_{i,j} -\frac{1}{2}) \log q_{i,j}  \\
%\end{eqnarray*}
%
\end{problem}{}

\end{document}
